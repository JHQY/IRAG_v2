# '''Text embedding placeholder'''
# import torch
# from sentence_transformers import SentenceTransformer
# from config.settings import settings
# import numpy as np

# class Embedder:
#     def __init__(self):
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.model = SentenceTransformer(settings.EMBEDDING_MODEL_NAME, device=device)

#     def embed_text(self, texts):
#         return np.array(self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False),dtype=np.float32)
    
#     def embed_query(self, query):
#         return np.array(self.model.encode([query], convert_to_numpy=True, show_progress_bar=False),dtype=np.float32)[0]()  # Load environment variables

"""
Embedder for IRAG multi-modal pipeline
- æ–‡æœ¬ â†’ BGE / SentenceTransformer embedding
- è¡¨æ ¼ â†’ TAPAS embedding (structure-aware)
"""

import torch
from transformers import AutoTokenizer, AutoModel
from transformers import TapasTokenizer, TapasModel
import numpy as np
import pandas as pd


class Embedder:
<<<<<<< HEAD
    def __init__(self):
        device = "cpu"
        if torch.cuda.is_available():
            try:
                _ = torch.randn(1, device="cuda")
                device = "cuda"
            except Exception:
                device = "cpu"
        self.model = SentenceTransformer(settings.EMBEDDING_MODEL_NAME, device=device)
=======
>>>>>>> 7baeaa1 (new-version-with-table)

    def __init__(self):

        # ----- æ–‡æœ¬æ¨¡å‹ï¼ˆBGEã€m3ã€sentence-BERTéƒ½å¯ä»¥ï¼‰ -----
        self.text_model_name = "BAAI/bge-m3"
        self.text_tokenizer = AutoTokenizer.from_pretrained(self.text_model_name)
        self.text_model = AutoModel.from_pretrained(self.text_model_name)
        self.text_model.eval()

        # ----- è¡¨æ ¼æ¨¡å‹ï¼ˆTAPAS è®ºæ–‡çº§è¡¨æ ¼ embeddingï¼‰ -----
        self.table_model_name = "google/tapas-base"
        self.table_tokenizer = TapasTokenizer.from_pretrained(self.table_model_name)
        self.table_model = TapasModel.from_pretrained(self.table_model_name)
        self.table_model.eval()

        # æ˜¯å¦ä½¿ç”¨ GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.text_model.to(self.device)
        self.table_model.to(self.device)



    # ----------------------------------------------------------------------
    # æ–‡æœ¬ embedding
    # ----------------------------------------------------------------------
    def embed_text(self, texts):
<<<<<<< HEAD
        return np.array(self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False),dtype=np.float32)
    
    def embed_query(self, query):
        return np.array(self.model.encode([query], convert_to_numpy=True, show_progress_bar=False),dtype=np.float32)[0]  # Load environment variables
=======
        """
        è¾“å…¥: texts = [str, str, ...]
        è¾“å‡º: np.ndarray (N, dim)
        """

        inputs = self.text_tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            outputs = self.text_model(**inputs)
            embeddings = outputs.last_hidden_state[:, 0]   # CLS embedding

        return embeddings.cpu().numpy()



    # ----------------------------------------------------------------------
    # è¡¨æ ¼ embeddingï¼ˆè®ºæ–‡çº§ TAPASï¼‰
    # ----------------------------------------------------------------------
    def embed_table(self, headers, rows):
        """
        è¾“å…¥:
            headers = ["Benefit", "Coverage", ...]
            rows = [
                ["Major Illness", "100%", "0"],
                ["Minor Illness", "30%", "200"]
            ]

        è¾“å‡º: np.ndarray (dim=768)
        """

        

        # --- 1. æ„é€  DataFrameï¼ˆTAPAS éœ€è¦ï¼‰ ---
        df = pd.DataFrame(rows, columns=headers)

        # TAPAS å¿…é¡»æœ‰ queries å‚æ•°
        inputs = self.table_tokenizer(
            table=df,
            queries=["table embedding query"],
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            outputs = self.table_model(**inputs)
            embedding = outputs.pooler_output  # shape (1, 768)

        return embedding.cpu().numpy()[0]

    def embed_query_table(self, query: str):
        """
        å°† query è½¬æˆ TAPAS æ‰€éœ€çš„ DataFrame æ ¼å¼
        """

        # ç”¨ DataFrame æ›´å®‰å…¨
        df = pd.DataFrame({"QUERY": [query]})

        inputs = self.table_tokenizer(
            table=df,                     # ğŸ‘ˆ NOW DataFrame
            queries=["query"],            # TAPAS å¿…å¡«
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            outputs = self.table_model(**inputs)
            emb = outputs.pooler_output  # (1, dim)

        return emb.cpu().numpy()[0]

>>>>>>> 7baeaa1 (new-version-with-table)
